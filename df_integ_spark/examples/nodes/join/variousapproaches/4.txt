    Column column = emp.col("deptId").equalTo(dept.col("id"));
    Column column1 = emp.col("age").gt(19);
    Dataset<Row> joined = emp.join(dept,column
      ,"left").where(column1);
    joined.explain(true );

Result:
============
{"age":50,"deptId":1,"name":"Michael","dname":"dept1","id":1}
{"age":30,"deptId":2,"name":" Andy "}
filtered justin due to age

Plan
===================    

== Parsed Logical Plan ==
'Filter (age#0L > 19)
+- Join LeftOuter, (deptId#1L = id#8L)
   :- Relation[age#0L,deptId#1L,name#2] json
   +- Relation[dname#7,id#8L] json

== Analyzed Logical Plan ==
age: bigint, deptId: bigint, name: string, dname: string, id: bigint
Filter (age#0L > cast(19 as bigint))
+- Join LeftOuter, (deptId#1L = id#8L)
   :- Relation[age#0L,deptId#1L,name#2] json
   +- Relation[dname#7,id#8L] json

== Optimized Logical Plan ==
Join LeftOuter, (deptId#1L = id#8L)
:- Filter (isnotnull(age#0L) && (age#0L > 19))
:  +- Relation[age#0L,deptId#1L,name#2] json
+- Relation[dname#7,id#8L] json

== Physical Plan ==
*BroadcastHashJoin [deptId#1L], [id#8L], LeftOuter, BuildRight
:- *Project [age#0L, deptId#1L, name#2]
:  +- *Filter (isnotnull(age#0L) && (age#0L > 19))
:     +- *FileScan json [age#0L,deptId#1L,name#2] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/manoharm/infoworks/spark/examples/input/emp.json], PartitionFilters: [], PushedFilters: [IsNotNull(age), GreaterThan(age,19)], ReadSchema: struct<age:bigint,deptId:bigint,name:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))
   +- *FileScan json [dname#7,id#8L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/manoharm/infoworks/spark/examples/input/dept.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dname:string,id:bigint>