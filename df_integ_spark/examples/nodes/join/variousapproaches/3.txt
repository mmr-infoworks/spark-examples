
Code
=========
Column column = emp.col("deptId").equalTo(dept.col("id"));
    Column column1 = emp.col("age").gt(19);
    Dataset<Row> joined = emp.join(dept,column.$amp$amp(column1)
      ,"left");
    joined.explain(true);

Result
=============

{"age":50,"deptId":1,"name":"Michael","dname":"dept1","id":1}
{"age":30,"deptId":2,"name":" Andy "}
{"age":19,"deptId":6,"name":"Justin"} // did not filter age < 19 
Plans
==============
== Parsed Logical Plan ==
Join LeftOuter, ((deptId#1L = id#8L) && (age#0L > cast(19 as bigint)))
:- Relation[age#0L,deptId#1L,name#2] json
+- Relation[dname#7,id#8L] json

== Analyzed Logical Plan ==
age: bigint, deptId: bigint, name: string, dname: string, id: bigint
Join LeftOuter, ((deptId#1L = id#8L) && (age#0L > cast(19 as bigint)))
:- Relation[age#0L,deptId#1L,name#2] json
+- Relation[dname#7,id#8L] json

== Optimized Logical Plan ==
Join LeftOuter, ((age#0L > 19) && (deptId#1L = id#8L))
:- Relation[age#0L,deptId#1L,name#2] json
+- Relation[dname#7,id#8L] json

== Physical Plan ==
*BroadcastHashJoin [deptId#1L], [id#8L], LeftOuter, BuildRight, (age#0L > 19)
:- *FileScan json [age#0L,deptId#1L,name#2] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/manoharm/infoworks/spark/examples/input/emp.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,deptId:bigint,name:string>
+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, true]))
   +- *FileScan json [dname#7,id#8L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/manoharm/infoworks/spark/examples/input/dept.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dname:string,id:bigint>