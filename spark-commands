
spark.read.option("header", true).option("inferSchema", true).orc("/iw/testmm/input/customer.orc")


spark.sql("create table customer_hivesql (ziw_row_id string ,ziw_scn decimal(38,18),ziw_source_start_date date ,ziw_source_start_timestamp timestamp ,ziw_target_start_timestamp timestamp ,ziw_target_start_date date ,ziw_source_end_date date ,ziw_source_end_timestamp timestamp ,ziw_target_end_date date ,ziw_target_end_timestamp timestamp ,ziw_active string ,ziw_is_deleted string ,ziw_status_flag string ,c_customer_sk decimal(38,0) ,c_customer_id string ,c_current_cdemo_sk decimal(38,0) ,c_current_hdemo_sk decimal(38,0) ,c_current_addr_sk decimal(38,0) ,c_first_shipto_date_sk decimal(38,0),c_first_sales_date_sk decimal(38,0) ,c_salutation string ,c_first_name string ,c_last_name string ,c_birth_day decimal(38,0) ,c_birth_month decimal(38,0) ,c_birth_year decimal(38,0) ,c_birth_country string ,c_login string ,c_email_address string ,c_last_review_date string) PARTITIONED BY (c_preferred_cust_flag string) stored as orc LOCATION '/iw/testmm/output/customer_hivesql'")

import org.apache.spark.sql.SaveMode;
spark.sql("SET hive.exec.dynamic.partition = true")
spark.sql("SET hive.exec.dynamic.partition.mode = nonstrict ")
spark.sql("SET hive.exec.max.dynamic.partitions.pernode = 400")
bucketBy(10, "num1")
cust.write.mode(SaveMode.Overwrite).bucketBy(2,"ziw_source_start_date").insertInto("customer_hivesql")
cust.write.mode(SaveMode.Overwrite).insertInto("customer_hivesql")



////Save as table with spark
cust.write.options(Map("path" -> "/iw/testmm/output/customer1")).partitionBy("ziw_row_id").bucketBy(2,"ziw_source_start_date").format("orc").saveAsTable("customertable1")
